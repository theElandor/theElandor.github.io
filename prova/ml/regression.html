<!doctype html>
<html lang="en">
<head>
<title><b>Logistic Regression</b></title>
<!-- 2023-10-06 ven 21:30 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="generator" content="Org-mode">
<meta name="author" content="Matteo Eros Lugli">

<link  href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.5/css/bootstrap.min.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.5/js/bootstrap.min.js"></script>
<style>
/* org mode styles on top of twbs */

html {
    position: relative;
    min-height: 100%;
}

body {
    font-size: 18px;
    margin-bottom: 105px;
}

footer {
    position: absolute;
    bottom: 0;
    width: 100%;
    height: 101px;
    background-color: #f5f5f5;
}

footer > div {
    padding: 10px;
}

footer p {
    margin: 0 0 5px;
    text-align: center;
    font-size: 16px;
}

#table-of-contents {
    margin-top: 20px;
    margin-bottom: 20px;
}

blockquote p {
    font-size: 18px;
}

pre {
    font-size: 16px;
}

.footpara {
    display: inline-block;
}

figcaption {
  font-size: 16px;
  color: #666;
  font-style: italic;
  padding-bottom: 15px;
}

/* from twbs docs */

.bs-docs-sidebar.affix {
    position: static;
}
@media (min-width: 768px) {
    .bs-docs-sidebar {
        padding-left: 20px;
    }
}

/* All levels of nav */
.bs-docs-sidebar .nav > li > a {
    display: block;
    padding: 4px 20px;
    font-size: 14px;
    font-weight: 500;
    color: #999;
}
.bs-docs-sidebar .nav > li > a:hover,
.bs-docs-sidebar .nav > li > a:focus {
    padding-left: 19px;
    color: #A1283B;
    text-decoration: none;
    background-color: transparent;
    border-left: 1px solid #A1283B;
}
.bs-docs-sidebar .nav > .active > a,
.bs-docs-sidebar .nav > .active:hover > a,
.bs-docs-sidebar .nav > .active:focus > a {
    padding-left: 18px;
    font-weight: bold;
    color: #A1283B;
    background-color: transparent;
    border-left: 2px solid #A1283B;
}

/* Nav: second level (shown on .active) */
.bs-docs-sidebar .nav .nav {
    display: none; /* Hide by default, but at >768px, show it */
    padding-bottom: 10px;
}
.bs-docs-sidebar .nav .nav > li > a {
    padding-top: 1px;
    padding-bottom: 1px;
    padding-left: 30px;
    font-size: 12px;
    font-weight: normal;
}
.bs-docs-sidebar .nav .nav > li > a:hover,
.bs-docs-sidebar .nav .nav > li > a:focus {
    padding-left: 29px;
}
.bs-docs-sidebar .nav .nav > .active > a,
.bs-docs-sidebar .nav .nav > .active:hover > a,
.bs-docs-sidebar .nav .nav > .active:focus > a {
    padding-left: 28px;
    font-weight: 500;
}

/* Nav: third level (shown on .active) */
.bs-docs-sidebar .nav .nav .nav {
    padding-bottom: 10px;
}
.bs-docs-sidebar .nav .nav .nav > li > a {
    padding-top: 1px;
    padding-bottom: 1px;
    padding-left: 40px;
    font-size: 12px;
    font-weight: normal;
}
.bs-docs-sidebar .nav .nav .nav > li > a:hover,
.bs-docs-sidebar .nav .nav .nav > li > a:focus {
    padding-left: 39px;
}
.bs-docs-sidebar .nav .nav .nav > .active > a,
.bs-docs-sidebar .nav .nav .nav > .active:hover > a,
.bs-docs-sidebar .nav .nav .nav > .active:focus > a {
    padding-left: 38px;
    font-weight: 500;
}

/* Show and affix the side nav when space allows it */
@media (min-width: 992px) {
    .bs-docs-sidebar .nav > .active > ul {
        display: block;
    }
    /* Widen the fixed sidebar */
    .bs-docs-sidebar.affix,
    .bs-docs-sidebar.affix-bottom {
        width: 213px;
    }
    .bs-docs-sidebar.affix {
        position: fixed; /* Undo the static from mobile first approach */
        top: 20px;
    }
    .bs-docs-sidebar.affix-bottom {
        position: absolute; /* Undo the static from mobile first approach */
    }
    .bs-docs-sidebar.affix .bs-docs-sidenav,.bs-docs-sidebar.affix-bottom .bs-docs-sidenav {
        margin-top: 0;
        margin-bottom: 0
    }
}
@media (min-width: 1200px) {
    /* Widen the fixed sidebar again */
    .bs-docs-sidebar.affix-bottom,
    .bs-docs-sidebar.affix {
        width: 263px;
    }
}
</style>
<script>
$(function() {
    'use strict';

    $('.bs-docs-sidebar li').first().addClass('active');

    $(document.body).scrollspy({target: '.bs-docs-sidebar'});

    $('.bs-docs-sidebar').affix();
});
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  displayAlign: "center",
  displayIndent: "2em",
  messageStyle: "none",
  "HTML-CSS": {
    scale: 100,
    styles: {
      ".MathJax_Display": {
        "font-size": "100%"
      }
    }
  },
  "SVG": {
    scale: 100,
    styles: {
      ".MathJax_SVG_Display": {
        "font-size": "100%",
        "margin-left": "-2.281em"
      }
    }
  }
});
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_SVG"></script>
</head>
<body>
<div id="content" class="container">
<div class="row"><div class="col-md-3 col-md-push-9"><nav id="table-of-contents">
<div id="text-table-of-contents" class="bs-docs-sidebar">
<ul class="nav">
<li><a href="#sec-1">1. Intro</a></li>
<li><a href="#sec-2">2. Binary case</a></li>
<li><a href="#sec-3">3. Multiclass case</a>
<ul class="nav">
<li><a href="#sec-3-1">3.1. Learning</a></li>
<li><a href="#sec-3-2">3.2. Geometry</a></li>
<li><a href="#sec-3-3">3.3. Performance</a></li>
</ul>
</li>
</ul>
</div>
</nav>
</div><div class="col-md-9 col-md-pull-3"><h1 class="title"><b>Logistic Regression</b></h1>
<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> Intro</h2>
<div class="outline-text-2" id="text-1">
<p>
Sometimes it is not necessary to calculate the likelyhood and then get the posterior via Bayes Rule.
Eventually you can directly model the posterior with some specific methods. One of theese is the
logistic regression. Basically you model your prior as function based on some paramteres \(\theta\), 
then you try to minimize the error made by this function comparing the predictions on the 
labels with the training data.
By modelling the prior directly, you only have \(P(Y=c|X=x)\), so you cannot sample from this distribution.
It means that if the model cannot output a element \(x\) given a class \(c\), because we would need the 
likelyhood for that. Logistic regression is a <b>Probabilistic discriminative classifier</b>.
</p>
<hr >
</div>
</div>
<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> Binary case</h2>
<div class="outline-text-2" id="text-2">
\begin{align}
P(Y=0|x) &= \frac{e^{w^{T}x + b}}{1+e^{w^{T}x + b}} \\
P(Y=1|x) &= \frac{1}{1+e^{w^{T}x + b}}
\end{align}
<p>
We can compute the decision boundary, which is linear in x.
</p>
\begin{align}
P(Y=0|x) &= P(Y=1|x) \\
P(Y=0|x) &- P(Y=1|x) = 0 \\
\log{P(Y=0|x)} &- \log{P(Y=1|x)} = 0\\
\vdots \\
w^{T}x+b &= 0
\end{align}
<hr >
</div>
</div>
<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3"><span class="section-number-2">3</span> Multiclass case</h2>
<div class="outline-text-2" id="text-3">
<p>
We need a set of weights \(w\) for each class. Actually, we can save 1 set of weights that we
don't need for the last class, that is the complementar of the rest.
For each class we also need the bias. In the equations, \(P(Y=K|x)\) is the probability of the
sample of not being assigned to class \(c\).
</p>
\begin{align}
P(Y=c|x) &= \frac{e^{w_{c}^{T}x + b_{c}}}{1+\sum_{l=1}^{K-1}e^{w_{l}^{T}x + b_{l}}} \\
P(Y=K|x) &= \frac{1}{1+\sum_{l=1}^{K-1}e^{w_{l}^{T}x + b_{l}}}
\end{align}
<p>
The classification function is
</p>
\begin{equation}
f_{LR}(x) = argmax_{c}P(Y=c|x)
\end{equation}
<p>
Considering a practical example, with 5 classes and 3 features the
logistic regression needs 5*(3-2)+2 = 12 paramters (2 vector of size 5 
of weights plus 2 biases).
</p>
</div>
<div id="outline-container-sec-3-1" class="outline-3">
<h3 id="sec-3-1"><span class="section-number-3">3.1</span> Learning</h3>
<div class="outline-text-3" id="text-3-1">
<p>
The learning part in the logistic regression consists in maximizing the so called
<b>log likelihood</b>. So the idea is to find the best set of parameters \(\theta\) that
fit well the training dataset:
</p>
\begin{equation}
argmax_{\theta}L(\theta|D) = argmax_{\theta}\sum_{i=1}^{n}\log{P(Y = y_{i}|X=x_{i})}
\end{equation}
<p>
In the <b>binary case</b> maximizing that function is the same as minimizing the negative binary cross entropy:
</p>
\begin{align}
&L(\theta|D) = \sum_{i=1}^{N}y_{i}\log{P(y=1|x)} + (1-y_{i})\log{(P(y=0|x))}\\
&L(\theta|D) = \sum_{i=1}^{N}y_{i}w^{T}x_{i} - \log{(1+e^{w^{T}x_{i}})}
\end{align}

<p>
Minimizing the function means computing the derivative of a summation, which is not ideal. 
What we are doing with linear regression is trying to minimize the <b>log likelihood for each one of the samples (this way we remove the summation).</b>
So we evaluate the log likelihood in a point, we compute the gradient in that point and tweak the parameters a little bit to go in the opposite
direction of the gradient. We go in the opposite direction of the gradient because if the gradient 
is positive it means that the function is increasing, so we want to go in the opposite direction
and vice-versa.
</p>

<p>
So let's compute the derivative of \(L(x_{i},\theta|D)\) with respect to a generic \(w_{k}\),
which is the weight of a feature. Remember that we are doing this fixing a single sample \(x_{i}\) to
make the formula easier.
</p>
\begin{align}
L(x_{i},\theta|D) &= y_{i}w^{T}x_{i} - \log{(1+e^{w^{T}x_{i}})} \\
\frac{d}{d_{w_{k}}}L(x_{i},\theta|D) &= \frac{d}{d_{w_{k}}}y_{i}w^{t}x_{i} - \frac{d}{d_{w_{k}}}\log{(1+e^{w^{T}x_{i}})} \\
&= \frac{d}{d_{w_{k}}}y_{i}(w_{1}x_{i,1} + w_{2}x_{i,2} + \cdots + w_{k}x_{i,k} + \cdots + w_{n}x_{i,n}) - \frac{d}{d_{w_{k}}}\log{(1+e^{w^{T}x_{i}})} \\
&= x_{i,k}y_{i} - \frac{d}{d_{w_{k}}}\log{(1+e^{w^{T}x_{i}})} \\
&= x_{i,k}y_{i} - \frac{1}{1+e^{w^{T}x_{i}}}\cdot e^{w^{T}x_{i}}\cdot x_{i,k}\\
&= x_{i,k}y_{i} - \frac{1}{1+e^{w^{T}x_{i}}}\cdot e^{w^{T}x_{i}}\cdot x_{i,k}\\
&= \bigl(y_{i} - \frac{e^{w^{T}x_{i}}}{1+e^{w^{T}x_{i}}} \bigr)x_{i,k}
\end{align}
<p>
Gradient descent update rule:
</p>
\begin{equation}
\theta_{j} = \theta_{j} - \alpha\frac{d}{\theta_{j}}f(\theta,x)
\end{equation}
<p>
where \(\alpha\) is the learning rate, and controls the size of the step.
For convex functions, the gradient descent will converge for sure to the
global minimum.
</p>
<ul class="org-ul">
<li><b>batch</b> gradient descent: all of the training examples are used at each iteration;
</li>
<li><b>minibatch</b> gradient descent: only a small subset of the training set is used;
</li>
<li><b>stochastic</b> gradient descent: just a random sample is used;
</li>
</ul>
<p>
The learning algorithm is the following:
<img src="./reg.png" class="img-responsive" alt="reg.png">
</p>
</div>
</div>
<div id="outline-container-sec-3-2" class="outline-3">
<h3 id="sec-3-2"><span class="section-number-3">3.2</span> Geometry</h3>
<div class="outline-text-3" id="text-3-2">
<ul class="org-ul">
<li><b>linear decision boundary</b> in the binary case. In the multiclass
case it is piecewise linear;
</li>
<li>Logistic regression has the same representation capacity of the LDA.
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-3-3" class="outline-3">
<h3 id="sec-3-3"><span class="section-number-3">3.3</span> Performance</h3>
<div class="outline-text-3" id="text-3-3">
<ul class="org-ul">
<li>Logistic regression always has <b>less</b> \((O(DC))\) parameters then LDA(\(O(DC+D^{2})\));
</li>
<li>LR is <b>faster then NB and LDA in classification</b> but slower in training,
because numerical optimization is needed;
</li>
<li>a
</li>
</ul>
</div>
</div>
</div>
</div></div></div>
<footer id="postamble" class="">
<div><p class="author">Author: Matteo Eros Lugli</p>
<p class="date">Created: 2023-10-06 ven 21:30</p>
<p class="creator"><a href="http://www.gnu.org/software/emacs/">Emacs</a> 27.1 (<a href="http://orgmode.org">Org-mode</a> 9.3)</p>
</div>
</footer>
</body>
</html>