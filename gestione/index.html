<!DOCTYPE html>
<html lang="en">
<head>
  
    <title>Gestione dell&#39;informazione ITA-(2022/2023) :: ML</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="gestione dell&#39;informazione" />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<link rel="canonical" href="/gestione/" />




<link rel="stylesheet" href="/assets/style.css">

  <link rel="stylesheet" href="/assets/blue.css">



<link rel="stylesheet" href="/style.css">


<link rel="apple-touch-icon" href="/img/apple-touch-icon-192x192.png">

  <link rel="shortcut icon" href="/img/favicon/blue.png">



<meta name="twitter:card" content="summary" />

  
    <meta name="twitter:site" content="" />
  
    <meta name="twitter:creator" content="" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="Gestione dell&#39;informazione ITA-(2022/2023)">
<meta property="og:description" content="gestione dell&#39;informazione" />
<meta property="og:url" content="/gestione/" />
<meta property="og:site_name" content="ML" />

  
    <meta property="og:image" content="/img/favicon/blue.png">
  

<meta property="og:image:width" content="2048">
<meta property="og:image:height" content="1024">


  <meta property="article:published_time" content="2023-06-20 16:30:40 &#43;0200 CEST" />












</head>
<body class="blue">


<div class="container center headings--one-size">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="/">
  <div class="logo">
    Home
  </div>
</a>

    </div>
    
      <div class="menu-trigger">menu</div>
    
  </div>
  
    <nav class="menu">
  <ul class="menu__inner menu__inner--desktop">
    
      
        
          <li><a href="/about">About</a></li>
        
      
        
          <li><a href="/htb">HTB</a></li>
        
      
      
        <ul class="menu__sub-inner">
          <li class="menu__sub-inner-more-trigger">Uni-Notes ▾</li>

          <ul class="menu__sub-inner-more hidden">
            
              
                <li><a href="/gestione">Inform. Retrieval</a></li>
              
            
              
                <li><a href="/compilatori">Linguaggi e Compilatori</a></li>
              
            
              
                <li><a href="/ml">Machine Learning and Deep Learning</a></li>
              
            
              
                <li><a href="/reti">Reti</a></li>
              
            
          </ul>
        </ul>
      
    

    
  </ul>

  <ul class="menu__inner menu__inner--mobile">
    
      
        <li><a href="/about">About</a></li>
      
    
      
        <li><a href="/htb">HTB</a></li>
      
    
      
        <li><a href="/gestione">Inform. Retrieval</a></li>
      
    
      
        <li><a href="/compilatori">Linguaggi e Compilatori</a></li>
      
    
      
        <li><a href="/ml">Machine Learning and Deep Learning</a></li>
      
    
      
        <li><a href="/reti">Reti</a></li>
      
    
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
<div class="post">
  <h1 class="post-title">
    <a href="/gestione/">Gestione dell&rsquo;informazione ITA-(2022/2023)</a></h1>
  <div class="post-meta">
    
      <span class="post-date">
        2023-06-20
        
      </span>
    
    
    
  </div>

  
  


  

  <div class="post-content"><div>
        <h2 id="cosa-posso-trovare-qui"><em>Cosa posso trovare qui?</em><a href="#cosa-posso-trovare-qui" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>Questa pagina (in italiano) contiene i miei appunti del corso di
&ldquo;Gestione dell&rsquo;informazione&rdquo; del corso di laurea triennale in informatica
di Unimore.
Verranno quindi riportati gli approfondimenti/chiarimenti che vengono
fatti durante le lezioni. Il materiale non è sostitutivo alle slide del corso.</p>
<p>→ Link per accedere al <code>materiale didattico</code>:</p>
<p><a href="https://moodle.unimore.it/course/view.php?id=7287">https://moodle.unimore.it/course/view.php?id=7287</a></p>
<h2 id="troubleshooting"><em>Troubleshooting</em><a href="#troubleshooting" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>Se la pagina non carica correttamente i contenuti in latec, basta <code>ricaricare</code> la pagina
una o al massimo un paio di volte. Questo problema può essere legato alla grande
quantità di testo e immagini presenti in questa sezione.</p>
<hr>
<h2 id="ir--table-of-contents">(IR) TABLE OF CONTENTS<a href="#ir--table-of-contents" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<ul>
<li><a href="#information-retrieval-vs-data-retrieval">Information Retrieval vs Data Retrieval</a></li>
<li><a href="#text-operations">Text Operations</a></li>
<li><a href="#full-text-indexing">Full Text Indexing</a></li>
<li><a href="#modelling">Modelling</a></li>
<li><a href="#evaluation">Evaluation</a></li>
<li><a href="#tolerant-retrieval">Tolerant Retrieval</a></li>
<li><a href="#ranking-the-web">Ranking the Web</a></li>
</ul>
<hr>
<h2 id="information-retrieval-vs-data-retrieval">Information Retrieval vs Data Retrieval<a href="#information-retrieval-vs-data-retrieval" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>Già dal titolo del si può intuire la principale differenza tra le due discipline: nell&rsquo;information retrieval
il focus è sulle <strong>informazioni</strong>, mentre in basi di dati il focus è sui <strong>dati</strong>. Ciò significa che in I.R
si lavora per soddisfare le <strong>esigenze informative</strong> dell&rsquo;utente, che possono essere più o meno articolate
e/o difficili da interpretare. Al contrario in basi di dati l&rsquo;utente ha il compito di descrivere esattamente
le sue necessità attraverso una <strong>query in linguaggio di interrogazione</strong>, la quale viene processata e vengono
restituiti gli oggetti che &ldquo;matchano&rdquo; esattamente la richiesta. In information retrieval si lavora con corpus
costituiti da documenti in linguaggio naturale, quindi si parla di dati <strong>non strutturati</strong>, mentre in data retrieval
si lavora solitamente con database relazionali (quindi strutturati).
L&rsquo;output del sistema è molto diverso nei due casi:</p>
<ul>
<li>I sistemi di I.R puntano a restituire un <strong>ranking</strong> che sia il più pertinente possibile alle richieste dell&rsquo;utente,
cercando di restituire nelle prime posizioni documenti rilevanti;</li>
<li>I sistemi di D.R restituiscono un pool di oggetti che semplicemente &ldquo;matchano&rdquo; la query inserita dall&rsquo;utente.</li>
</ul>
<h2 id="text-operations">Text Operations<a href="#text-operations" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<h3 id="document-processing">Document Processing<a href="#document-processing" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<ul>
<li>
<p><strong><code>Analisi lessicale</code></strong>: si converte una sequenza di caratteri in una sequenza
di token, dei potenziali canditati per dei termini &ldquo;index&rdquo;. Questo compito viene
eseguito dal <strong>lexer</strong> (es. GNU lex).</p>
</li>
<li>
<p><strong><code>Eliminazione delle stopwords</code></strong>: vengono eliminate quelle parole &ldquo;inutili&rdquo;
che non danno informazioni per la ricerca, esistono elenchi appositi di stopwords
per ogni lingua.</p>
</li>
<li>
<p><strong><code>Stemming e Lemmatization</code></strong>: esempio di uno <strong>stem</strong> &ndash;&gt; &lsquo;connect&rsquo; è lo stem
di parole come &lsquo;connected&rsquo;, &lsquo;connection&rsquo;, &lsquo;connecting&rsquo;. Lo <strong>stemmer</strong> è
il tool che si occupa della stemmization.</p>
<p>Esistono alcuni parametri per giudicare uno stemmer: (i) <strong>correttezza</strong>, (ii) <strong>efficacia</strong>,
(iii) miglioramento della <strong>performance</strong>.</p>
<p>esempio di un <strong>lemma</strong> &ndash;&gt; &lsquo;see&rsquo; è il lemma per &lsquo;seen&rsquo; &lsquo;saw&rsquo;; Il <strong>lemmatizer</strong>
si occupa della generazione dei lemma.</p>
</li>
<li>
<p><strong><code>Selezione degli Index</code></strong> vengono scelti i token più
significativi (attraverso l&rsquo;uso di parsers e taggers). Può essere fatta manualmente (da esperti) oppure automaticamente.
Esempio: &ldquo;Say&rdquo; &ldquo;Chair&rdquo; &ldquo;Be&rdquo; &ldquo;Enough&rdquo; &ndash;&gt; &ldquo;Chair&rdquo;.</p>
</li>
<li>
<p><strong><code>Parsing</code></strong>: processo di analizzare uno stream di dati input e verificare la sua
correttezza sintattica. Questi strumenti (detti parser) lavorano sulla base
di &ldquo;banche dati&rdquo; usando un approccio statistico. I parser analizzano l&rsquo;intera
struttura sintattica della frase e generano un&rsquo;altra struttura dati che descrive
il ruolo di ogni token (es. Abstract Syntax Tree).</p>
</li>
<li>
<p><strong><code>Tagging</code></strong>: processo che assegna alle parole il loro ruolo all&rsquo;interno della frase.</p>
</li>
</ul>
<p>(Verbo, Nome, Aggettivo, ecc&hellip;)</p>
<h3 id="thesauri">Thesauri<a href="#thesauri" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>Un <strong>Thesaurus</strong> è una lista di parole (sinonimi e contrari) importanti in un dato domain di conoscenza.
Ad esempio, in ambito medico, alla parola &ldquo;hand&rdquo; potrebbero essere collegati concetti
come &ldquo;sanityzers&rdquo;, &ldquo;transplantation&rdquo;, ecc..
A livello strutturale sono quindi dei <em>dizionari</em> che contengono associazioni.</p>
<h3 id="python-nltk-the-basics">Python NLTK, the basics<a href="#python-nltk-the-basics" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>In questo script vengono trattati i seguenti argomenti:
(i) Generazione dei token, (ii) Rimozione delle stopwords, (iii) Lemmatizzazione,
(iv) Stemming, (v) Tagging.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> nltk
<span style="color:#f92672">from</span> nltk.corpus <span style="color:#f92672">import</span> stopwords
<span style="color:#f92672">from</span> nltk.stem.porter <span style="color:#f92672">import</span> PorterStemmer
<span style="color:#f92672">from</span> nltk.stem.lancaster <span style="color:#f92672">import</span> LancasterStemmer

text <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;This is a tests&#34;</span>
tokens <span style="color:#f92672">=</span> nltk<span style="color:#f92672">.</span>word_tokenize(text)
print(tokens)

<span style="color:#75715e"># Stopwords removal and Lemmatization</span>
wnl <span style="color:#f92672">=</span> nltk<span style="color:#f92672">.</span>WordNetLemmatizer()
<span style="color:#75715e"># per stampare tutte le parole che non sono stopwords.</span>
<span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> tokens:
	<span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> t <span style="color:#f92672">in</span> stopwords<span style="color:#f92672">.</span>words(<span style="color:#e6db74">&#39;english&#39;</span>):
		print(wnl<span style="color:#f92672">.</span>lemmatize(t))

<span style="color:#75715e"># --&gt; [&#39;This&#39;, &#39;is&#39;, &#39;a&#39;, &#39;tests&#39;]</span>
<span style="color:#75715e">#     This</span>
<span style="color:#75715e">#     test</span>


<span style="color:#75715e"># Stemming, using Porter and Lancaster, two popular stemmers</span>
<span style="color:#75715e"># They give the same output for easy input</span>
porter <span style="color:#f92672">=</span> PorterStemmer()
print([porter<span style="color:#f92672">.</span>stem(t) <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> tokens])

lancaster <span style="color:#f92672">=</span> LancasterStemmer()
print([lancaster<span style="color:#f92672">.</span>stem(t) <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> tokens])

<span style="color:#75715e"># POS tagging a list of lemmatizen tokens!</span>
print([nltk<span style="color:#f92672">.</span>pos_tag([wnl<span style="color:#f92672">.</span>lemmatize(t) <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> tokens])])
<span style="color:#75715e"># --&gt; [[(&#39;This&#39;, &#39;DT&#39;), (&#39;is&#39;, &#39;VBZ&#39;), (&#39;a&#39;, &#39;DT&#39;), (&#39;test&#39;, &#39;NN&#39;)]]</span>
</code></pre></div><p><strong>Esercizio 1:</strong></p>
<p>eseguire le seguenti operazioni: (i)Tokenization, (ii)Elimination of stopwords, (iii)Stemming, (iv) Selection of nouns
su un file .txt contenente un libro di testo.
In questo caso ho scaricato una copia locale del file, per semplicità.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> nltk
<span style="color:#f92672">from</span> nltk.corpus <span style="color:#f92672">import</span> stopwords
<span style="color:#f92672">from</span> nltk.stem.porter <span style="color:#f92672">import</span> PorterStemmer

path <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;./book.txt&#34;</span>
file <span style="color:#f92672">=</span> open(path)
raw <span style="color:#f92672">=</span> file<span style="color:#f92672">.</span>read()
print(len(raw))

<span style="color:#75715e">#step1: tokenization</span>
tokens <span style="color:#f92672">=</span> nltk<span style="color:#f92672">.</span>word_tokenize(raw)
print(tokens[:<span style="color:#ae81ff">20</span>])

<span style="color:#75715e">#step2: eliminate stopwords</span>
no_stop_tokens <span style="color:#f92672">=</span> [t <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> tokens <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> t <span style="color:#f92672">in</span> stopwords<span style="color:#f92672">.</span>words(<span style="color:#e6db74">&#39;english&#39;</span>)]
print(len(no_stop_tokens))

<span style="color:#75715e">#step3: stemming</span>
porter <span style="color:#f92672">=</span> PorterStemmer()
stemmed_tokens <span style="color:#f92672">=</span> ([porter<span style="color:#f92672">.</span>stem(t) <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> no_stop_tokens])

<span style="color:#75715e">#step4: get nouns thanks to tagging</span>
tagged_tokens <span style="color:#f92672">=</span> (nltk<span style="color:#f92672">.</span>pos_tag([t <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> stemmed_tokens]))
print(tagged_tokens[:<span style="color:#ae81ff">20</span>])
nouns <span style="color:#f92672">=</span> [t[<span style="color:#ae81ff">0</span>] <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> tagged_tokens <span style="color:#66d9ef">if</span> t[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;NN&#39;</span>]
print(nouns[:<span style="color:#ae81ff">20</span>])
</code></pre></div><h3 id="wordnet--thesaurus">Wordnet (Thesaurus)<a href="#wordnet--thesaurus" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p><code>Synset</code> &ndash;&gt; concetto, può essere rappresentato da più parole.
In un generico thesaurus sono chiamati <em>Thesaurus Index Term</em>.</p>

  <figure class="left" >
    <img src="/ox-hugo/thes.png"   />
    
  </figure>


<p>Wordnet mette a disposizione una serie di relazioni tra i synset,
come i seguenti:</p>

  <figure class="left" >
    <img src="/ox-hugo/relazioni.png"   />
    
  </figure>


<p>La relazioni solitamente più usate sono <strong>Hypernymy</strong> e <strong>Meronymy</strong>.
Usare un thesaurus non è sempre una scelta corretta. In
un search engine generico come google non ha senso, dato che il
grafo delle relazioni diventerebbe enorme senza portare grossi benefici.
Al contrario è molto utile in casi specifici (e.g. search engine per
paper in ambito medico). Per corpus generici solitamente si usa un approccio
più naive ma che comunque è efficiente: il <strong>bag of words</strong> (vedi dopo).</p>
<h3 id="word-similarities">Word similarities<a href="#word-similarities" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p><strong><code>sinonimia</code></strong> &ndash;&gt; relazione binaria che lega due parole attraverso
il significato.</p>
<p><strong><code>similarità o distanza</code></strong> &ndash;&gt; metrica più &ldquo;loose&rdquo; per dare un gradiente
di similarità.</p>
<p>Queste relazioni non vanno confuse con le relazioni del thesaurus.
Ad esempio benzina e macchina potrebbero essere collegate nel thesaurus
ma potrebbero anche non essere sinonimi!</p>
<p>Come misurare la similarità tra concetti (tramite l&rsquo;uso di un thesaurus)? (<em>i</em>)path based (<em>ii</em>)information content measures.</p>
<ul>
<li><strong>Path based</strong>: sfrutto la gerarchia di ipernimia per stabilire il &ldquo;livello&rdquo; di
somiglianza di due concetti. Esistono principalmente 2 formule per effettuare
il calcolo: (<em>i</em>) path distance similarity, (<em>ii</em>) Wu-Palmer similarity.</li>
</ul>

  <figure class="left" >
    <img src="/ox-hugo/pathdistance.png"   />
    
      <figcaption class="center" ><span class="figure-number">Figure 1: </span>Path based similarities</figcaption>
    
  </figure>


<p><strong><code>Path distance</code></strong> similarity:</p>
<p>\begin{equation}
sim_{path-distance}(c_{1}, c_{2})= \frac{1}{ShortestPath(c_{1}, c_{2})+1}
\end{equation}</p>
<p><strong><code>Wu-Palmer</code></strong> similarity:</p>
<p>\begin{equation}
sim_{Wu-Palmer}(c_{1}, c_{2})= \frac{2*depth(LCS(c_{1},c_{2}))}{depth(c_{1})+depth(c_{2})}
\end{equation}</p>
<p>Il problema della prima formula è che è molto discontinua
e genera dei valori non ben distribuiti. Questo problema
viene risolto da Wu-Palmer con la loro formula.
Per chiarezza, LCS &ndash;&gt; Least Common Subsumer.</p>
<ul>
<li>
<p><strong>Information content mesaures</strong>: quanto spesso questi
concetti vengono usati nello stesso contesto.
Dato un concetto, definisco con P(c) la probabilità
che scelta una parola a caso  in un corpus rappresenti
il determinato concetto.</p>
<p><strong>Information content:</strong> Può essere interpretato
come il <em>livello di sorpresa</em> di un particolare concetto. Questo indice
infatti è alto quando c&rsquo;è un synset &ldquo;raro&rdquo; o &ldquo;inaspettato&rdquo;.</p>
<p>\begin{equation}
IC( c) = -log P( c)
\end{equation}</p>
</li>
</ul>
<p>Similarità di <strong><code>Resnik</code></strong>: calcolo l&rsquo;information content del
least common subsumer. Due concetti sono molto simili se
si incontra raramente un termine il cui senso è comune tra i
due.</p>
<p>\begin{equation}
sim_{resnik}(c_{1}, c_{2}) = -log(P(LCS(c1,c2)))
\end{equation}</p>
<h3 id="word-sense-disambiguation">Word Sense disambiguation<a href="#word-sense-disambiguation" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>La WSD consiste essenzialmente nell&rsquo;assegnare il senso corretto ad ogni
istanza di una certa parola di interesse.</p>
<ol>
<li>Determinare tutti i sensi che quella parola può assumere: abbastanza
automatico usando un thesaurus;</li>
<li>Analizzare il contesto dove la parola compare.
<ul>
<li><strong>Bag of words</strong>: il <em>contesto</em> è rappresentato da un pool di parole
&ldquo;vicine&rdquo; al termine di interesse che vengono estratte.</li>
<li><strong>Relational information:</strong> approccio più complesso che estrae altri
parametri come la distanza.</li>
</ul>
</li>
</ol>
<p>Ecco un&rsquo;approccio per implementare il tutto:</p>
<pre tabindex="0"><code class="language-example" data-lang="example">for each noun N, for each sense Sn of N:
  compute confidence Csn in choosing sn as sense of N
select sense with higher confidence
</code></pre><p>Ora la domanda è: come calcolo la <code>confidence</code> \(C_{s_{n}}\) ?</p>
<p>\(\rightarrow\) mi baso sulla similarità tra \(s_{n}\) e tutti gli altri sensi delle parole
nel contesto! Poi per calcolare la similitudine posso usare tecniche come la <em>path-based</em>,
già discussa in precedenza. Testualmente può anche essere espresso nel seguente modo:
Voglio cercare di trovare il senso della parola S che &ldquo;fitta&rdquo; meglio nel contesto di riferimento,
che è costituito da molte parole, anch&rsquo;esse legate a molti sensi.</p>
<p>Ricapitolando, ecco lo pseudocodice dell&rsquo;algoritmo:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">max_confidence <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
<span style="color:#66d9ef">for</span> each Si synset di I:
  confidence <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
  <span style="color:#66d9ef">for</span> each J term <span style="color:#f92672">in</span> context:
	max <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
	<span style="color:#66d9ef">for</span> each Sj synset of J:
	  similarity <span style="color:#f92672">=</span> simil(Sj, Si)
	  <span style="color:#66d9ef">if</span>(similarity <span style="color:#f92672">&gt;</span> max):
		max <span style="color:#f92672">=</span> similarity
	confidence <span style="color:#f92672">+=</span> max
  <span style="color:#66d9ef">if</span>(confidence <span style="color:#f92672">&gt;</span> max_confidence):
	max_confidence <span style="color:#f92672">=</span> confidence
pick(synseth_with_confidence(max_confidence))
</code></pre></div><p>Codice effettivo di un semplice word sense disambiguator in python:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">disambiguateTerms</span>(terms):
  <span style="color:#66d9ef">for</span> t_i <span style="color:#f92672">in</span> terms: <span style="color:#75715e"># t_i is target term</span>
	selSense <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
	selScore <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
	<span style="color:#66d9ef">for</span> s_ti <span style="color:#f92672">in</span> wn<span style="color:#f92672">.</span>synsets(t_i, wn<span style="color:#f92672">.</span>NOUN):
	  score_i <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
	  <span style="color:#66d9ef">for</span> t_j <span style="color:#f92672">in</span> terms: <span style="color:#75715e"># t_j term in t_i&#39;s context window</span>
		<span style="color:#66d9ef">if</span> (t_i<span style="color:#f92672">==</span>t_j):
	<span style="color:#66d9ef">continue</span>
  bestScore <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
	<span style="color:#66d9ef">for</span> s_tj <span style="color:#f92672">in</span> wn<span style="color:#f92672">.</span>synsets(t_j, wn<span style="color:#f92672">.</span>NOUN):
	  tempScore <span style="color:#f92672">=</span> s_ti<span style="color:#f92672">.</span>wup_similarity(s_tj)
	  <span style="color:#66d9ef">if</span> (tempScore<span style="color:#f92672">&gt;</span>bestScore):
		bestScore<span style="color:#f92672">=</span>tempScore
	score_i <span style="color:#f92672">=</span> score_i <span style="color:#f92672">+</span> bestScore
	  <span style="color:#66d9ef">if</span> (score_i<span style="color:#f92672">&gt;</span>selScore):
		selScore <span style="color:#f92672">=</span> score_i
		selSense <span style="color:#f92672">=</span> s_ti
	<span style="color:#66d9ef">if</span> (selSense <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>):
	  print(t_i,<span style="color:#e6db74">&#34;: &#34;</span>,selSense,<span style="color:#e6db74">&#34;, &#34;</span>,selSense<span style="color:#f92672">.</span>definition())
	  print(<span style="color:#e6db74">&#34;Score: &#34;</span>,selScore)
	<span style="color:#66d9ef">else</span>:
	  print(t_i,<span style="color:#e6db74">&#34;: --&#34;</span>)
</code></pre></div><h3 id="query-expansion">Query Expansion<a href="#query-expansion" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>Esercizio sull&rsquo;espansione di query.
Proposta di algoritmo / soluzione:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> nltk
<span style="color:#f92672">from</span> nltk.corpus <span style="color:#f92672">import</span> stopwords
<span style="color:#f92672">from</span> nltk.stem.porter <span style="color:#f92672">import</span> PorterStemmer
<span style="color:#f92672">from</span> nltk.corpus <span style="color:#f92672">import</span> wordnet <span style="color:#66d9ef">as</span> wn

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">disambiguateTerms</span>(terms):
	selected_syns <span style="color:#f92672">=</span> []
	syns_defs <span style="color:#f92672">=</span> []
	<span style="color:#66d9ef">for</span> t_i <span style="color:#f92672">in</span> terms: <span style="color:#75715e"># t_i is target term</span>
		selSense <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
		selScore <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
		<span style="color:#66d9ef">for</span> s_ti <span style="color:#f92672">in</span> wn<span style="color:#f92672">.</span>synsets(t_i, wn<span style="color:#f92672">.</span>NOUN):
			score_i <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
			<span style="color:#66d9ef">for</span> t_j <span style="color:#f92672">in</span> terms: <span style="color:#75715e"># t_j term in t_i&#39;s context window</span>
				<span style="color:#66d9ef">if</span> (t_i<span style="color:#f92672">==</span>t_j):
					<span style="color:#66d9ef">continue</span>
				bestScore <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
				<span style="color:#66d9ef">for</span> s_tj <span style="color:#f92672">in</span> wn<span style="color:#f92672">.</span>synsets(t_j, wn<span style="color:#f92672">.</span>NOUN):
					tempScore <span style="color:#f92672">=</span> s_ti<span style="color:#f92672">.</span>wup_similarity(s_tj)
					<span style="color:#66d9ef">if</span> (tempScore<span style="color:#f92672">&gt;</span>bestScore):
						bestScore<span style="color:#f92672">=</span>tempScore
				score_i <span style="color:#f92672">=</span> score_i <span style="color:#f92672">+</span> bestScore
			<span style="color:#66d9ef">if</span> (score_i<span style="color:#f92672">&gt;</span>selScore):
				selScore <span style="color:#f92672">=</span> score_i
				selSense <span style="color:#f92672">=</span> s_ti
		<span style="color:#66d9ef">if</span> (selSense <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>):
			selected_syns<span style="color:#f92672">.</span>append(selSense)
			syns_defs<span style="color:#f92672">.</span>append(selSense<span style="color:#f92672">.</span>definition())
	<span style="color:#66d9ef">return</span> selected_syns, syns_defs

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">expand_query</span>(syns):
	res <span style="color:#f92672">=</span> []
	<span style="color:#66d9ef">for</span> s <span style="color:#f92672">in</span> syns:
		<span style="color:#66d9ef">for</span> l <span style="color:#f92672">in</span> s<span style="color:#f92672">.</span>lemmas():
			  res<span style="color:#f92672">.</span>append(l<span style="color:#f92672">.</span>name())
	<span style="color:#66d9ef">return</span> res


raw <span style="color:#f92672">=</span> input (<span style="color:#e6db74">&#34;Insert query: &#34;</span>)
tokens <span style="color:#f92672">=</span> nltk<span style="color:#f92672">.</span>word_tokenize(raw)
nostokens <span style="color:#f92672">=</span> [t <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> tokens <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> t <span style="color:#f92672">in</span> stopwords<span style="color:#f92672">.</span>words(<span style="color:#e6db74">&#39;english&#39;</span>)]
tagged_tokens <span style="color:#f92672">=</span> (nltk<span style="color:#f92672">.</span>pos_tag([t <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> nostokens]))
pool <span style="color:#f92672">=</span> [wn<span style="color:#f92672">.</span>morphy(x[<span style="color:#ae81ff">0</span>]) <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> tagged_tokens]
print(pool)

selected_syns, syns_def <span style="color:#f92672">=</span> disambiguateTerms(pool)
expanded_query <span style="color:#f92672">=</span> expand_query(selected_syns)
print(expanded_query)
</code></pre></div><hr>
<h2 id="full-text-indexing">Full Text Indexing<a href="#full-text-indexing" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<h3 id="introduzione">Introduzione<a href="#introduzione" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>Solitamente un indice è appropriato quando:</p>
<ol>
<li>Ho molti dati da gestire;</li>
<li>Quando ho un pool di dati semi-statico, quindi può essere aggiornato
periodicamente ma non supporta frequenze troppo alte.</li>
</ol>
<p>Gli index più usati sono (<em>i</em>) inverted index, (<em>ii</em>) suffix array, (<em>iii</em>) signature
files. Ovviamente si cercano sempre di capire i costi di accesso, di costruzione e
di aggiornamento.</p>
<ul>
<li>\(n\) &ndash;&gt; dimensione in byte occupati dal testo;</li>
<li>\(m\) &ndash;&gt; lunghezza del pattern che voglio cercare;</li>
<li>\(M\) &ndash;&gt; byte disponibili in memoria principale;</li>
</ul>
<h3 id="trie">TrIe<a href="#trie" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>(prefix tree)
Indice che memorizza stringhe, per velocizzare la ricerca e altre operazioni.</p>

  <figure class="left" >
    <img src="/ox-hugo/trie.png"   />
    
      <figcaption class="center" ><span class="figure-number">Figure 2: </span>Trie, prefix tree</figcaption>
    
  </figure>


<p>Costo di ricerca di una parola: \(O(m)\), quindi lineare rispetto al numero di
caratteri della parola che sto cercando.</p>
<h3 id="inverted-index">Inverted Index<a href="#inverted-index" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>L&rsquo;idea è quella di memorizzare, per ogni termine \(t\), una lista di tutti i
documenti che contengono quella parola. Ecco le componenti pricipali di
un inverted index:</p>
<ol>
<li><strong><code>Vocabolario</code></strong> &ndash;&gt; insieme di tutte le parole contenute nella collezione di documenti.
Per ogni parola viene mantenuta come informazione il numero di documenti in cui
quella parola è presente (<em>document frequency</em>).</li>
<li><strong><code>Posting List</code></strong> &ndash;&gt; mantiene le informazioni riguardo alla precisa locazione
delle singole parole. Esistono due tipi di posting list:
<ul>
<li><strong>Document based</strong>: esempio &ndash;&gt; car:11 (4) &ndash;&gt; viene indicato <em>in che documento</em>
compare la parola e con quale frequenza.</li>
<li><strong>Word based</strong>: ho la <em>locazione</em> della parola di interesse nel documento,
a livello posizionale.</li>
</ul>
</li>
</ol>
<p>Qui sotto sono illustrati degli esempi di record che potrebbero essere presenti
in un inverted index. Notare come la document frequency coincida con il numero
di caselle presenti nella posting list. Questo è il caso in cui la posting
list sia <em>Document based</em>. Se fosse <em>Word Based</em>, ovviamente i nodi della
posting list sarebbero più ingombranti: invece della frequenza all&rsquo;interno
del documento, sarebbero indicate le precise posizioni delle istanze.</p>
<table>
<thead>
<tr>
<th>Index term</th>
<th>df</th>
<th>posting-list-couple</th>
<th>posting-list-couple</th>
</tr>
</thead>
<tbody>
<tr>
<td>computer</td>
<td>2</td>
<td>\(D_{7},4\)</td>
<td>\(D_{8},2\)</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Index term</th>
<th>df</th>
<th>posting-list-couple</th>
<th>posting-list-couple</th>
<th>posting-list-couple</th>
</tr>
</thead>
<tbody>
<tr>
<td>database</td>
<td>3</td>
<td>\(D_{1},2\)</td>
<td>\(D_{2},3\)</td>
<td>\(D_{3},2\)</td>
</tr>
</tbody>
</table>
<p>Lo space requirment del vocabolario è abbastanza esiguo: esiste una legge che
descrive il suo andamento chiamata <code>Heaps law</code>. L&rsquo;andamento è \(O(n^{b})\) con \(b \in [0,1]\)
Ovviamente la grandezza del vocabolario dipende da tutto il preprocessing che
viene fatto, quindi da come vengono generati i token e come vengono filtrati.</p>
<p>Il costo di memorizzazione dell&rsquo;inverted index è generalmente \(O(n)\):</p>
<ul>
<li>Per i document based, è circa il 20%-40% della dimensione del testo;</li>
<li>Per i word based, è il 40% senza stopwords e l'80% con le stopwords;</li>
</ul>
<h3 id="costruzione-di-un-inverted-index">Costruzione di un Inverted Index<a href="#costruzione-di-un-inverted-index" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>Solitamente la costruzione si basa sulla memorizzazione del vocabolario
in un <strong>trie</strong>. Dopo aver fatto il preprocessing si seguono i seguenti
step:</p>
<ol>
<li>Leggere la parola del testo</li>
<li>Cercare la parola nel trie</li>
<li>Se non è presente, viene aggiunta, altrimenti viene
aggiornata la lista delle occorrenze di quella parola.</li>
</ol>
<p>Costo: \(O(n)\), dove n è la somma dei token da indicizzare.
Normalmente l&rsquo;inverted index viene diviso in due file, per dividere
il dizionario dalle posting list. Questo perchè il dizionario viene
caricato in memoria principale, mentre le posting list rimangono
in memoria secondaria e vengono accedute mediante degli offset.
Nel file del vocabolario vengono memorizzate le parole, e per ogni parola
un puntatore alla propria lista all&rsquo;interno del posting file.
Ecco un&rsquo;esempio di entry all&rsquo;interno di un vocabulary file:</p>
<ul>
<li>&ldquo;start&rdquo; indica l&rsquo;offset di inizio della lista;</li>
<li>&ldquo;n&rdquo; indica la lunghezza della lista corrispondente;</li>
</ul>
<table>
<thead>
<tr>
<th>Term</th>
<th>Start</th>
<th>n</th>
</tr>
</thead>
<tbody>
<tr>
<td>science</td>
<td>7</td>
<td>3</td>
</tr>
</tbody>
</table>
<p>Quando devo usare il vocabolario, in che struttura dati conviene caricarlo per
velocizzare la ricerca?
Per chiarezza: il trie di cui parlavamo prima viene usato in fase di
<strong>costruzione</strong>, per facilitarla. Una volta finita la fase di costruzione,
il file con il vocabolario viene memorizzato e il trie non serve più.
Rimane quindi da capire quale sia la struttura più efficiente in cui
<strong>caricare</strong>, in seguito, il vocabolario.</p>
<ol>
<li><strong>Sorted Array</strong> &ndash;&gt; ricerca binaria \((log(n))\). L&rsquo;inserimento è costoso, dato
che può comportare lo shifting degli elementi all&rsquo;interno della struttura.</li>
<li><strong>B+ Tree</strong> &ndash;&gt; veloce e efficiente ma comporta spazio extra, richiede
la gestione di una struttura dati aggiuntiva.</li>
<li><strong>Trie</strong></li>
<li><strong>Hash</strong></li>
</ol>
<h3 id="ricerca">Ricerca<a href="#ricerca" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p><strong><code>Boolean Retrieval</code></strong>
La boolean retrieval è semplice concettualmente, basta ottenere i risultati (insiemi di documenti )
dei singoli elementi e combinarli usando l&rsquo;apposito operatore booleano (unione, intersezione, ecc&hellip;).</p>
<p>Per quanto riguarda il processing dell&rsquo;operando <strong>AND</strong>, si può usare un semplice algoritmo di &ldquo;merging&rdquo; per
risolvere il problema.
Ad esempio, si supponga di voler cercare tutti i documenti che contengono entrambe
le parole &ldquo;computer&rdquo; e &ldquo;science&rdquo;.
In tal caso, supponendo che le due posting list contengano gli ID
dei documenti in maniera <strong>ordinata</strong>, basta un&rsquo;approccio a due
puntatori per risolvere il problema in \(O(n)\).</p>
<ul>
<li>\(p1\) &ndash;&gt; puntatore al primo elemento della posting list di &ldquo;computer&rdquo;, che contiene
la lista (ordinata) di documenti in cui compare la parola;</li>
<li>\(p2\) &ndash;&gt; puntatore al primo elemento della posting list di &ldquo;science&rdquo;;</li>
</ul>
<!-- raw HTML omitted -->
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">answer <span style="color:#f92672">=</span> []
<span style="color:#66d9ef">while</span> p1 <span style="color:#f92672">!=</span> NULL <span style="color:#f92672">and</span> p2 <span style="color:#f92672">!=</span> NULL:
  <span style="color:#66d9ef">if</span> (doc_ID(p1) <span style="color:#f92672">==</span> doc_ID(p2)):
	answer<span style="color:#f92672">.</span>append(doc_ID(p1))
  <span style="color:#66d9ef">else</span> <span style="color:#66d9ef">if</span> doc_ID(p1) <span style="color:#f92672">&lt;</span> doc_ID(p2):
	p1 <span style="color:#f92672">=</span> p1<span style="color:#f92672">-&gt;</span>next;
 <span style="color:#66d9ef">else</span>
   p2 <span style="color:#f92672">=</span> p2<span style="color:#f92672">-&gt;</span>next;
<span style="color:#66d9ef">return</span> answer;
</code></pre></div><p><strong><code>Phrasal Retrieval</code></strong></p>
<p>Spesso è interessante sapere, ad esempio, in quali documenti e in che
posizione appare una determinata <strong>frase</strong>, quindi una sequenza di parole.
Ad esempio, si può essere interessati a capire in che posizione e in quali
documenti appare la stringa &ldquo;computer science&rdquo;. Come intuibile, non basta
sapere se in un documento appaiano entrambe, serve anche controllare che esse
siano contigue (in questo caso).
Per eseguire in maniera efficiente questa operazione è opportuno avere
un inverted index <strong>word based</strong>.
Si seguono i seguenti passi:</p>
<ol>
<li>Si crea una lista di documenti che contengono enrambe le parole, come
visto nel paragrafo precedente.</li>
<li>Si selezionano, uno alla volta, tutti i documenti di questa lista;</li>
<li>Per ogni documento, si creano \(n\) vettori, dove \(n\) indica il numero
di parole della frase (nel caso dell&rsquo;esempio, \(n=2\));
Ogni vettore contiene tutti gli <em>indici di apparizione</em> di quella
parola in quel documento;</li>
<li>Si cercano le tuple corrette all&rsquo;interno dei vettori. Nell&rsquo;esempio,
si vogliono individuare tutti gli indici \(i\) tali che
\(Computer_{i}+1 = Science_{j}\)</li>
</ol>
<p>Vettore delle apparizioni di &ldquo;Computer&rdquo; nel documento \(D_{1}\) \(\downarrow\)</p>
<table>
<thead>
<tr>
<th>i=0</th>
<th>i=1</th>
<th>i=2</th>
<th>i=3</th>
</tr>
</thead>
<tbody>
<tr>
<td>4</td>
<td>8</td>
<td>12</td>
<td>44</td>
</tr>
</tbody>
</table>
<p>Vettore delle apparizioni di &ldquo;Science&rdquo; sempre nel documento \(D_{1}\) \(\downarrow\)</p>
<table>
<thead>
<tr>
<th>i=0</th>
<th>i=1</th>
<th>i=2</th>
<th>i=3</th>
</tr>
</thead>
<tbody>
<tr>
<td>6</td>
<td>9</td>
<td>32</td>
<td>45</td>
</tr>
</tbody>
</table>
<p>In questo caso, la frase &ldquo;computer science&rdquo; compare agli indici 1 e 3,
perchè i valori corrispondenti sono contigui!
Ecco lo pseudocodice, da applicare ad ogni documento \(d \in D\):</p>
<ul>
<li>\(P\) = insieme di tutti gli array del documento corrente;</li>
<li>\(k_{i}\) = keyword i-esima;</li>
<li>\(P_{i}\) = array delle apparizioni di \(k_{i}\);</li>
<li>\(P_{s}\) = array delle apparizioni più corto;</li>
<li>\(k_{s}\) = keyword che compare meno volte;</li>
</ul>
<p>In questo caso, si intende semplicemente ritornare la lista dei documenti
che contengono la frase cercata.</p>
<ul>
<li>(p+i-s) \(\rightarrow\) si aggiunge alla posizione \(p\) all&rsquo;interno dell&rsquo;array, la distanza tra \(i\) (indice all&rsquo;interno della frase della parola
su cui si sta facendo la binary search) e \(s\) (indice all&rsquo;interno della frase della parola di riferimento).
Essenzialmente quindi si aggiunge la distanza tra la parola di riferimento e la parola di cui si sta cercando l&rsquo;occorrenza.</li>
</ul>
<!-- raw HTML omitted -->
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">for</span> each position p <span style="color:#f92672">in</span> Ps:
  <span style="color:#66d9ef">for</span> each keyword ki <span style="color:#f92672">!=</span> ks: <span style="color:#75715e"># per tutte le altre keyword della frase</span>
	use BinarySearch to find a position (p<span style="color:#f92672">+</span>i<span style="color:#f92672">-</span>s) <span style="color:#f92672">in</span> Pi
  <span style="color:#66d9ef">if</span> correct position <span style="color:#66d9ef">for</span> each keyword found, add d to answer <span style="color:#75715e"># d is current document</span>
  <span style="color:#66d9ef">else</span> <span style="color:#66d9ef">try</span> <span style="color:#66d9ef">with</span> next position, going on <span style="color:#66d9ef">with</span> loop
<span style="color:#66d9ef">return</span> answer
</code></pre></div><p>Questo procedimento è da iterare su ogni documento che contiene tutte le parole di riferimento, per capire se compaiono
nell&rsquo;ordine corretto.</p>
<h2 id="modelling">Modelling<a href="#modelling" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<ul>
<li>📃 <a href="https://theelandor.github.io/prova/gestione/021122.html">Lezione 02-11-22 e 03-11-22</a> (Modello Vettoriale)</li>
<li>📃 <a href="https://theelandor.github.io/prova/gestione/101122.html">Lezione 10-11-22</a> (Modello Probabilistico)</li>
<li>📃 <a href="https://theelandor.github.io/prova/gestione/171122.html">Lezione 17-11-22</a> (Modelli Fuzzy e Okami BM25)</li>
</ul>
<h2 id="evaluation">Evaluation<a href="#evaluation" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<ul>
<li>📃 <a href="https://theelandor.github.io/prova/gestione/071222.html">Lezione 07-12-22</a> (Retrieval Evaluation)</li>
</ul>
<h2 id="tolerant-retrieval">Tolerant Retrieval<a href="#tolerant-retrieval" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<ul>
<li>📃 <a href="https://theelandor.github.io/prova/gestione/tol.html">Tolerant Retrieval</a></li>
</ul>
<h2 id="ranking-the-web">Ranking the Web<a href="#ranking-the-web" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<ul>
<li>📃 <a href="https://theelandor.github.io/prova/gestione/web.html">Page Rank</a></li>
</ul>

      </div></div>

  

  
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  
</div>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>© 2024 Powered by <a href="http://gohugo.io">Hugo</a></span>
    
        <span>:: Theme made by <a href="https://twitter.com/panr">panr</a></span>
      </div>
  </div>
</footer>

<script src="/assets/main.js"></script>
<script src="/assets/prism.js"></script>







  
</div>

</body>
</html>
